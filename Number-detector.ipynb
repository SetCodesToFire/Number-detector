{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Loading MNIST DATASET into the project**\n",
    "\n",
    "\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits \n",
    "that is commonly used for training various image processing systems.[1][2] The database is also widely used for training and \n",
    "testing in the field of machine learning.It was created by \"re-mixing\" the samples from NIST's original dataset. The \n",
    "creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was \n",
    "taken from American high school students, it was not well-suited for machine learning experiments.[5] Furthermore, the black \n",
    "and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale \n",
    "levels.\n",
    "\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test \n",
    "set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were \n",
    "taken from NIST's testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate; \n",
    "one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of \n",
    "0.23 percent. The original creators of the database keep a list of some of the methods tested on it. In their original \n",
    "paper, they use a support vector machine to get an error rate of 0.8 percent. An extended dataset similar to MNIST called EMNIST \n",
    "has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)Starting the tensorflow interactive session**\n",
    "\n",
    "Importing the tensorflow into the project as well as creating a new tensorflow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Building a Softmax Regression Model**\n",
    "\n",
    "In this section we will build a softmax regression model with a single linear layer. In the next section, we will extend this to the case of softmax regression with a multilayer convolutional network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholders**\n",
    "\n",
    "We start building the computation graph by creating nodes for the input images and target output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here x and y_ aren't specific values. Rather, they are each a placeholder -- a value that we'll input when we ask TensorFlow to run a computation.\n",
    "\n",
    "The input images x will consist of a 2d tensor of floating point numbers. Here we assign it a shape of [None, 784], where 784 is the dimensionality of a single flattened 28 by 28 pixel MNIST image, and None indicates that the first dimension, corresponding to the batch size, can be of any size. The target output classes y_ will also consist of a 2d tensor, where each row is a one-hot 10-dimensional vector indicating which digit class (zero through nine) the corresponding MNIST image belongs to.\n",
    "\n",
    "The shape argument to placeholder is optional, but it allows TensorFlow to automatically catch bugs stemming from inconsistent tensor shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**\n",
    "\n",
    "We now define the weights W and biases b for our model. We could imagine treating these like additional inputs, but \n",
    "TensorFlow has an even better way to handle them: Variable. A Variable is a value that lives in TensorFlow's computation graph.\n",
    "It can be used and even modified by the computation. In machine learning applications, one generally has the model\n",
    "parameters be Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
